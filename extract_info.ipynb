{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor.orig_data_center import OrigBookDataCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdc = OrigBookDataCenter.build_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdc = OrigBookDataCenter.build_from_wayback_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset_from_json(filename):\n",
    "    with open(filename) as in_file:\n",
    "        return json.load(in_file)\n",
    "\n",
    "def load_dataset_from_jsonl(filename):\n",
    "    dataset = []\n",
    "    with open(filename) as in_file:\n",
    "        for line in in_file.readlines():\n",
    "            d = json.loads(line)\n",
    "            dataset.append(d)\n",
    "    return dataset\n",
    "\n",
    "def export_dataset_as_jsonl(data, filename):\n",
    "    with open(filename, 'w') as out_file:\n",
    "        for d in data:\n",
    "            json.dump(d, out_file)\n",
    "            out_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'data/v04/masked_char_data_filtered_by_name_and_description.jsonl'\n",
    "filename2 = 'data/v04/truncated_char_data_filtered_by_name_and_description.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_books = {}\n",
    "with open('list_literatures_info.txt') as in_f:\n",
    "    for line in in_f.readlines():\n",
    "        title, source, url = line.strip().split('|')\n",
    "        old_books[url] = (title, source)\n",
    "\n",
    "new_books = {}\n",
    "with open('list_literatures_info_clean.txt') as in_f:\n",
    "    for line in in_f.readlines():\n",
    "        title, source, url = line.strip().split('|')\n",
    "        new_books[url] = (title, source)\n",
    "\n",
    "book_mapping = {}\n",
    "for key, val in old_books.items():\n",
    "    try:\n",
    "        book_mapping[val] = new_books[key]\n",
    "    except Exception:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset_from_jsonl(filename1)\n",
    "literatures = set()\n",
    "characters = set()\n",
    "character_lists = set()\n",
    "for d in data:\n",
    "    book_title = d['book_title']\n",
    "    source = d['source']\n",
    "    char_name = d['character_name']\n",
    "\n",
    "    lit_key = (book_title, source)\n",
    "    new_book_title, new_source = book_mapping[lit_key]\n",
    "    char_key = (book_title, source, char_name)\n",
    "\n",
    "    summ_url = obdc.literatures[lit_key].summary_url\n",
    "    char_list_url = obdc.characters[char_key].character_list_url\n",
    "    desc_url = obdc.characters[char_key].description_url\n",
    "    literatures.add((new_book_title, new_source, summ_url))\n",
    "    characters.add((new_book_title, new_source, desc_url))\n",
    "    # if char_list_url is not None:\n",
    "    #     character_lists.add(char_list_url)\n",
    "literatures = sorted(list(literatures))\n",
    "# character_lists = sorted(list(character_lists))\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "out_f = open('list_literatures.txt', 'w')\n",
    "for info in literatures:\n",
    "    out_f.write('|'.join(info)+'\\n')\n",
    "out_f.close()\n",
    "\n",
    "out_f = open('list_characters.txt', 'w')\n",
    "for info in characters:\n",
    "    out_f.write('|'.join(info)+'\\n')\n",
    "out_f.close()\n",
    "\n",
    "# out_f = open('list_character_lists.txt', 'w')\n",
    "# for url in character_lists:\n",
    "#     out_f.write(url+'\\n')\n",
    "# out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "    while True:\n",
    "        try:\n",
    "            local_random = random.Random()\n",
    "            local_random.seed(datetime.now())\n",
    "            for _ in range(local_random.randint(20000000, 40000000)): pass\n",
    "            response = requests.get(\n",
    "                f'http://web.archive.org/wayback/available?url={url}&timestamp=202012',\n",
    "                # proxies={\n",
    "                #     \"http\": \"http://49b2b65fb41f4f089c4056b5abf63af7:@proxy.crawlera.com:8010/\",\n",
    "                # },\n",
    "            ).json()\n",
    "            snapshots = response['archived_snapshots']\n",
    "            if 'closest' in snapshots:\n",
    "                timestamp = snapshots['closest']['timestamp']\n",
    "                yyyy_mm = timestamp[:6]\n",
    "                if yyyy_mm >= '202009':\n",
    "                    return (True, snapshots['closest']['url'])\n",
    "            # os.system(f'/home/huangme-pop/anaconda3/envs/lcdata/bin/archiver {url}')\n",
    "            return (False, url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = []\n",
    "    missed_urls = missed_again_urls\n",
    "    missed_again_urls = []\n",
    "\n",
    "    for url in missed_urls:\n",
    "        futures.append(executor.submit(process_url, url=url))\n",
    "\n",
    "    pbar = tqdm(total=len(missed_urls))\n",
    "    count = 0\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        found, url = future.result()\n",
    "        if not found: missed_again_urls.append(url)\n",
    "        else: found_urls.append(url)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(literatures))):\n",
    "#     url = literatures[i]\n",
    "#     response = requests.get(f'https://archive.org/wayback/available?url={url}')\n",
    "#     response = json.loads(response.text)\n",
    "#     snapshots = response['archived_snapshots']\n",
    "#     if 'closest' in snapshots:\n",
    "#         timestamp = snapshots['closest']['timestamp']\n",
    "#         yyyy_mm = timestamp[:6]\n",
    "#         if yyyy_mm >= '202009': continue\n",
    "#     return_code = os.system(f'/home/huangme-pop/anaconda3/envs/lcdata/bin/archiver {url}')\n",
    "#     print(return_code)\n",
    "found_urls = list(set(found_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in missed_again_urls:\n",
    "#     print(url)\n",
    "\n",
    "print(len(missed_again_urls))\n",
    "print(len(found_urls))\n",
    "\n",
    "out_f = open('list_character_lists_cached.txt', 'w')\n",
    "for url in found_urls:\n",
    "    out_f.write(url+'\\n')\n",
    "out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "urls = {}\n",
    "with open('list_literatures_cached.txt') as in_f:\n",
    "    for line in in_f.readlines():\n",
    "        if len(line) == 0: continue\n",
    "        a = re.search(r'/(https?://.*$)', line)\n",
    "        url = a.group(1)\n",
    "        if url[-1] == '/': url = url[:-1]\n",
    "        urls[url] = line.strip()\n",
    "\n",
    "out_f = open('list_literatures_info.txt', 'w')\n",
    "with open('list_literatures.txt') as in_f:\n",
    "    for line in in_f.readlines():\n",
    "        try:\n",
    "            title, source, url = line.strip().split('|')\n",
    "            if url[-1] == '/': url = url[:-1]\n",
    "            out_f.write(f'{title}|{source}|{urls[url]}\\n')\n",
    "        except Exception:\n",
    "            print(line)\n",
    "out_f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_f = open('list_literatures_info_clean.txt', 'w')\n",
    "l = []\n",
    "with open('list_literatures_info.txt') as in_f:\n",
    "    for line in in_f.readlines():\n",
    "        info = line.strip().split('|')\n",
    "        l.append(info)\n",
    "l.sort()\n",
    "for info in l:\n",
    "    out_f.write('|'.join(info)+'\\n')\n",
    "out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}